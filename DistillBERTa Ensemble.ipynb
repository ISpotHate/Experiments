{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Transformers installation\n! pip install transformers datasets\n# To install from source instead of the last release, comment the command above and uncomment the following one.\n# ! pip install git+https://github.com/huggingface/transformers.git","metadata":{"id":"rMCcyRXuYs0D","outputId":"6076b273-b288-4abe-ef1e-f5dd8803c27f","execution":{"iopub.status.busy":"2022-08-11T17:01:09.351109Z","iopub.execute_input":"2022-08-11T17:01:09.351576Z","iopub.status.idle":"2022-08-11T17:01:22.206803Z","shell.execute_reply.started":"2022-08-11T17:01:09.351480Z","shell.execute_reply":"2022-08-11T17:01:22.205716Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.5.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (8.0.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb","metadata":{"id":"pRJfGRA6jy8Z","outputId":"f61bdfdf-da2a-4e2b-ef8f-4159bd541141","execution":{"iopub.status.busy":"2022-08-11T17:01:22.208563Z","iopub.execute_input":"2022-08-11T17:01:22.208889Z","iopub.status.idle":"2022-08-11T17:01:33.026998Z","shell.execute_reply.started":"2022-08-11T17:01:22.208859Z","shell.execute_reply":"2022-08-11T17:01:33.025848Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.12.21)\nRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.19.4)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.16.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.0.4)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.2.3)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.28.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.7.2)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.27)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.9)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.1.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.8.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!wandb login 8e626b5e08f18a0147c131286b8e06f99a2997f0","metadata":{"id":"7D6YiM60j05k","outputId":"1255bc92-d2ae-44a7-84d4-33cd3bb80934","execution":{"iopub.status.busy":"2022-08-11T17:06:15.067697Z","iopub.execute_input":"2022-08-11T17:06:15.068092Z","iopub.status.idle":"2022-08-11T17:06:17.817667Z","shell.execute_reply.started":"2022-08-11T17:06:15.068059Z","shell.execute_reply":"2022-08-11T17:06:17.816547Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"classla/FRENK-hate-en\")","metadata":{"id":"1bVKx-NHYvmK","outputId":"39116292-d198-4bc6-fa6e-1be01ab27b2e","execution":{"iopub.status.busy":"2022-08-11T17:06:21.179993Z","iopub.execute_input":"2022-08-11T17:06:21.180486Z","iopub.status.idle":"2022-08-11T17:06:23.134239Z","shell.execute_reply.started":"2022-08-11T17:06:21.180424Z","shell.execute_reply":"2022-08-11T17:06:23.133095Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c8bb8ae2d234ae096fa0f92f7572838"}},"metadata":{}}]},{"cell_type":"code","source":"dataset.remove_columns(['target', 'topic'])","metadata":{"id":"WTEJ6ZFsYxnR","outputId":"d732bdea-c395-4093-c440-ef2004088204","execution":{"iopub.status.busy":"2022-08-11T17:05:43.803599Z","iopub.execute_input":"2022-08-11T17:05:43.804357Z","iopub.status.idle":"2022-08-11T17:05:43.820167Z","shell.execute_reply.started":"2022-08-11T17:05:43.804311Z","shell.execute_reply":"2022-08-11T17:05:43.819002Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 8404\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 933\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 2301\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)","metadata":{"id":"oCHCcghBY1oa","outputId":"c1e75ec1-da01-4e2e-863b-c5c0ada76ea0","execution":{"iopub.status.busy":"2022-08-11T17:45:59.861075Z","iopub.execute_input":"2022-08-11T17:45:59.861766Z","iopub.status.idle":"2022-08-11T17:46:10.104860Z","shell.execute_reply.started":"2022-08-11T17:45:59.861722Z","shell.execute_reply":"2022-08-11T17:46:10.103758Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp34_ona6h\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f143e50d57a44579ba24cff813929df"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\ncreating metadata file for /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\nloading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-cased\",\n  \"activation\": \"gelu\",\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"vocab_size\": 28996\n}\n\nhttps://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsn4qovlh\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f88dd94d93594afd91d41889cd334568"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\ncreating metadata file for /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\nhttps://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_wd9lu7j\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"972f516c57f24e6a9987569296a07ebd"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\ncreating metadata file for /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\nloading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\nloading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\nloading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\nloading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-cased\",\n  \"activation\": \"gelu\",\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"vocab_size\": 28996\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2b54d5823bd4a379867e4a318afb837"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19b3663942284866978ffb65ff1b9d5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d84730810e02450f88c016d47a48f3e2"}},"metadata":{}}]},{"cell_type":"code","source":"small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))","metadata":{"id":"rp9ISy6LY4Zx","outputId":"b19b0f65-62b3-4263-d8f9-a39d450b6653","execution":{"iopub.status.busy":"2022-08-11T17:06:27.553901Z","iopub.execute_input":"2022-08-11T17:06:27.554321Z","iopub.status.idle":"2022-08-11T17:06:27.584987Z","shell.execute_reply.started":"2022-08-11T17:06:27.554286Z","shell.execute_reply":"2022-08-11T17:06:27.584134Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=5)","metadata":{"id":"_At5piWTY8yr","outputId":"1792c403-a3b8-429b-86ed-4bd611efe21d","execution":{"iopub.status.busy":"2022-08-11T17:46:20.088083Z","iopub.execute_input":"2022-08-11T17:46:20.088490Z","iopub.status.idle":"2022-08-11T17:46:23.213794Z","shell.execute_reply.started":"2022-08-11T17:46:20.088459Z","shell.execute_reply":"2022-08-11T17:46:23.212610Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-cased\",\n  \"activation\": \"gelu\",\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"vocab_size\": 28996\n}\n\nloading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\nSome weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom datasets import load_metric\n\nmetric = load_metric(\"accuracy\")","metadata":{"id":"0g2DVqrdZLmO","outputId":"1ff20309-5d1b-43e0-d509-f501243ebb57","execution":{"iopub.status.busy":"2022-08-11T17:07:51.326221Z","iopub.execute_input":"2022-08-11T17:07:51.326969Z","iopub.status.idle":"2022-08-11T17:07:52.581587Z","shell.execute_reply.started":"2022-08-11T17:07:51.326919Z","shell.execute_reply":"2022-08-11T17:07:52.580472Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df576788a20e4a36a2909979c4eaba9a"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"id":"OLGz_BC1ZMrz","execution":{"iopub.status.busy":"2022-08-11T17:07:52.583007Z","iopub.execute_input":"2022-08-11T17:07:52.583378Z","iopub.status.idle":"2022-08-11T17:07:52.589060Z","shell.execute_reply.started":"2022-08-11T17:07:52.583348Z","shell.execute_reply":"2022-08-11T17:07:52.588079Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", \n                                  evaluation_strategy=\"epoch\", \n                                  save_strategy='epoch',\n                                  load_best_model_at_end=True, \n                                  per_device_train_batch_size=8, \n                                  report_to=\"wandb\")","metadata":{"id":"17L8UTisZBhz","outputId":"1c0ccd58-9bea-4b27-82d3-db2f8a07f1ff","execution":{"iopub.status.busy":"2022-08-11T17:46:33.998770Z","iopub.execute_input":"2022-08-11T17:46:33.999715Z","iopub.status.idle":"2022-08-11T17:46:34.009193Z","shell.execute_reply.started":"2022-08-11T17:46:33.999669Z","shell.execute_reply":"2022-08-11T17:46:34.008319Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics\n)","metadata":{"id":"3UHiZSVFZzzL","execution":{"iopub.status.busy":"2022-08-11T17:46:36.414128Z","iopub.execute_input":"2022-08-11T17:46:36.414536Z","iopub.status.idle":"2022-08-11T17:46:36.451795Z","shell.execute_reply.started":"2022-08-11T17:46:36.414502Z","shell.execute_reply":"2022-08-11T17:46:36.450925Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"l9D8RsYra5EV","outputId":"8df8231c-728c-4bf2-c95a-2a15b1d6442c","execution":{"iopub.status.busy":"2022-08-11T17:46:38.649936Z","iopub.execute_input":"2022-08-11T17:46:38.650361Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, topic, token_type_ids, target. If text, topic, token_type_ids, target are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 100\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 39\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [39/39 09:11, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.846806</td>\n      <td>0.620000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.723675</td>\n      <td>0.620000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.693010</td>\n      <td>0.620000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, topic, token_type_ids, target. If text, topic, token_type_ids, target are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 8\nSaving model checkpoint to test_trainer/checkpoint-13\nConfiguration saved in test_trainer/checkpoint-13/config.json\nModel weights saved in test_trainer/checkpoint-13/pytorch_model.bin\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, topic, token_type_ids, target. If text, topic, token_type_ids, target are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 8\nSaving model checkpoint to test_trainer/checkpoint-26\nConfiguration saved in test_trainer/checkpoint-26/config.json\nModel weights saved in test_trainer/checkpoint-26/pytorch_model.bin\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, topic, token_type_ids, target. If text, topic, token_type_ids, target are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 8\nSaving model checkpoint to test_trainer/checkpoint-39\nConfiguration saved in test_trainer/checkpoint-39/config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"l8BE_oinj-73"},"execution_count":null,"outputs":[]}]}